{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb4espuLKJiA"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Hub Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2022-03-29T12:29:38.721776Z",
          "iopub.status.busy": "2022-03-29T12:29:38.721232Z",
          "iopub.status.idle": "2022-03-29T12:29:38.725154Z",
          "shell.execute_reply": "2022-03-29T12:29:38.724592Z"
        },
        "id": "jM3hCI1UUzar"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ6SNYq_tVVC"
      },
      "source": [
        "# Classify text with BERT\n",
        "\n",
        "This tutorial contains complete code to fine-tune BERT to perform sentiment analysis on a dataset of plain-text IMDB movie reviews.\n",
        "In addition to training a model, you will learn how to preprocess text into an appropriate format.\n",
        "\n",
        "In this notebook, you will:\n",
        "\n",
        "- Load the IMDB dataset\n",
        "- Load a BERT model from TensorFlow Hub\n",
        "- Build your own model by combining BERT with a classifier\n",
        "- Train your own model, fine-tuning BERT as part of that\n",
        "- Save your model and use it to classify sentences\n",
        "\n",
        "If you're new to working with the IMDB dataset, please see [Basic text classification](https://www.tensorflow.org/tutorials/keras/text_classification) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PHBpLPuQdmK"
      },
      "source": [
        "## About BERT\n",
        "\n",
        "[BERT](https://arxiv.org/abs/1810.04805) and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers. \n",
        "\n",
        "BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCjmX4zTCkRK"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:29:38.728372Z",
          "iopub.status.busy": "2022-03-29T12:29:38.727971Z",
          "iopub.status.idle": "2022-03-29T12:29:40.581474Z",
          "shell.execute_reply": "2022-03-29T12:29:40.580473Z"
        },
        "id": "q-YbjCkzw0yU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e3c9763-4f5d-40a8-c075-4bddf01c2708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 3.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 55.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q -U \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w_XlxN1IsRJ"
      },
      "source": [
        "You will use the AdamW optimizer from [tensorflow/models](https://github.com/tensorflow/models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:29:40.585969Z",
          "iopub.status.busy": "2022-03-29T12:29:40.585266Z",
          "iopub.status.idle": "2022-03-29T12:29:49.391942Z",
          "shell.execute_reply": "2022-03-29T12:29:49.391141Z"
        },
        "id": "b-P1ZOA0FkVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4856ba-0630-4650-d34c-2a77fb3f472c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |▌                               | 30 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█                               | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 81 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 92 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 112 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 122 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 133 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 143 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 153 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 163 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 174 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 184 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 194 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 204 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 215 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 225 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 235 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 245 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 256 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 266 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 276 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 286 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 296 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 307 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 317 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 327 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 337 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 348 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 358 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 368 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 378 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 389 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 399 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 409 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 419 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 430 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 440 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 450 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 460 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 471 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 481 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 491 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 501 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 512 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 522 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 532 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 542 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 552 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 563 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 573 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 583 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 593 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 604 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 614 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 624 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 634 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 645 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 655 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 665 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 675 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 686 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 696 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 706 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 716 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 727 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 737 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 747 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 757 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 768 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 778 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 788 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 798 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 808 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 819 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 829 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 839 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 849 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 860 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 870 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 880 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 890 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 901 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 911 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 921 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 931 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 942 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 952 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 962 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 972 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 983 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 993 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.7 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.7 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.7 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.7 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.7 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.7 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.7 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.8 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.8 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.8 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.8 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.8 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.8 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8 MB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 33.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 59.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 55.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 8.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 237 kB 46.5 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tf-models-official==2.7.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install translators"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBr3S_VzgE-g",
        "outputId": "ffa5e39d-aade-4133-ec57-7748bbfd0401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting translators\n",
            "  Downloading translators-5.1.1-py3-none-any.whl (25 kB)\n",
            "Collecting lxml>=4.8.0\n",
            "  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 8.1 MB/s \n",
            "\u001b[?25hCollecting PyExecJS>=1.5.1\n",
            "  Downloading PyExecJS-1.5.1.tar.gz (13 kB)\n",
            "Collecting requests>=2.27.1\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 758 kB/s \n",
            "\u001b[?25hCollecting pathos>=0.2.8\n",
            "  Downloading pathos-0.2.8-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting loguru>=0.6.0\n",
            "  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting ppft>=1.6.6.4\n",
            "  Downloading ppft-1.6.6.4-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.8->translators) (0.3.4)\n",
            "Collecting pox>=0.3.0\n",
            "  Downloading pox-0.3.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: multiprocess>=0.70.12 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.8->translators) (0.70.12.2)\n",
            "Requirement already satisfied: six>=1.7.3 in /usr/local/lib/python3.7/dist-packages (from ppft>=1.6.6.4->pathos>=0.2.8->translators) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (1.24.3)\n",
            "Building wheels for collected packages: PyExecJS\n",
            "  Building wheel for PyExecJS (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyExecJS: filename=PyExecJS-1.5.1-py3-none-any.whl size=14598 sha256=8671fd735e6509dda27d040fbd084f36e73b77995422815d3362d022d766849f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/ee/03/da5c0b4a8c13362beeb844eb913bbe58a89bde1de2b9157007\n",
            "Successfully built PyExecJS\n",
            "Installing collected packages: ppft, pox, requests, PyExecJS, pathos, lxml, loguru, translators\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed PyExecJS-1.5.1 loguru-0.6.0 lxml-4.8.0 pathos-0.2.8 pox-0.3.0 ppft-1.6.6.4 requests-2.27.1 translators-5.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:29:49.396206Z",
          "iopub.status.busy": "2022-03-29T12:29:49.395613Z",
          "iopub.status.idle": "2022-03-29T12:29:52.068483Z",
          "shell.execute_reply": "2022-03-29T12:29:52.067720Z"
        },
        "id": "_XgTpm9ZxoN9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load twitter datasets"
      ],
      "metadata": {
        "id": "pca41lyu3Cgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\"NOT\", \"OFF\"]\n",
        "list_of_labels = []\n",
        "data_set = []\n",
        "longest_line = 0\n",
        "\n",
        "with open(\"OLID_Tain_ATUSER_URL_EmojiRemoved_Pedro.txt\", 'r', encoding = 'utf-8') as f:\n",
        "  print(f.readline())\n",
        "  for lines in f:\n",
        "    line = f.readline().split(\" \")\n",
        "    line[0] = line[0].split(\"\\t\")[1]\n",
        "    temp = line[-1].split(\"\\t\")\n",
        "    temp[-1] = temp[-1].split(\"\\n\")[0]\n",
        "    line[-1] = temp[0]\n",
        "    # line = ' '.join(line)\n",
        "    if len(line) > longest_line:\n",
        "      longest_line = len(line)\n",
        "    list_of_labels.append(temp[1:])\n",
        "    data_set.append(line)\n",
        "    # print(line, temp[1:])\n",
        "print(\"longest line: \", longest_line)\n",
        "for i in range(len(data_set)):\n",
        "  while len(data_set[i]) < longest_line:\n",
        "    data_set[i].append(\" \")\n",
        "\n",
        "data_set = tf.convert_to_tensor(data_set)\n",
        "data_set = tf.data.Dataset.from_tensor_slices(data_set)\n",
        "\n",
        "\n",
        "for i in range(len(list_of_labels)):\n",
        "  if list_of_labels[i][0] == \"NOT\":\n",
        "    list_of_labels[i] = 0\n",
        "  else: \n",
        "    list_of_labels[i] = 1\n",
        "\n",
        "list_of_labels = tf.convert_to_tensor(list_of_labels)\n",
        "list_of_labels = tf.data.Dataset.from_tensor_slices(list_of_labels)\n",
        "\n",
        "for data, label in zip(enumerate(data_set), enumerate(list_of_labels)):\n",
        "  data = (data, label)\n",
        "# for i, data in enumerate(data_set):\n",
        "#   if list_of_labels[i][0] == \"NOT\":\n",
        "#     data = (data, tf.convert_to_tensor([0]))\n",
        "#   else:\n",
        "#     data = (data, tf.convert_to_tensor([1]))\n",
        "# labels = tf.convert_to_tensor(labels)\n",
        "print(data_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "1Pq02yBt5qNb",
        "outputId": "36406233-bdb8-4214-e93c-8b4fe4fe92bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6143b2c389c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlongest_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OLID_Tain_ATUSER_URL_EmojiRemoved_Pedro.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'OLID_Tain_ATUSER_URL_EmojiRemoved_Pedro.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_off = []\n",
        "labels_not = []\n",
        "data_set_off = []\n",
        "data_set_not = []\n",
        "longest_line = 0\n",
        "\n",
        "with open(\"OLID_Tain_ATUSER_URL_EmojiRemoved_Pedro.txt\", 'r', encoding = 'utf-8') as f:\n",
        "  f.readline()\n",
        "  for lines in f:\n",
        "    line = f.readline().split(\"\\t\", 1)\n",
        "    textAndLabels = line[1].split(\"\\t\", 1)\n",
        "    label = textAndLabels[1].split(\"\\t\")[0]\n",
        "    if label == \"OFF\":\n",
        "          labels_off.append(line[0])\n",
        "          data_set_off.append(textAndLabels[0])\n",
        "    elif label == \"NOT\":\n",
        "          labels_not.append(line[0])\n",
        "          data_set_not.append(textAndLabels[0])\n",
        "    else:\n",
        "      print(\"Somethings wrong\")\n",
        "    #line = textAndLabels[0]\n",
        "    #labels.append(label)\n",
        "    # line = ' '.join(line)\n",
        "    if len(textAndLabels[0]) > longest_line:\n",
        "      longest_line = len(textAndLabels[0])\n",
        "    #data_set.append(line)\n",
        "    # print(line, temp[1:])\n",
        "print(\"longest line: \", longest_line)\n",
        "for i in range(len(data_set_off)):\n",
        "  data_set_off[i].ljust(longest_line - len(data_set_off[i]))\n",
        "for i in range(len(data_set_not)):\n",
        "  data_set_not[i].ljust(longest_line - len(data_set_not[i]))\n",
        "\n",
        "print(labels_off)"
      ],
      "metadata": {
        "id": "XN0YE5Bhests",
        "outputId": "aa078bbc-00ce-4d42-a73a-35b73ef8ce5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "longest line:  329\n",
            "['90194', '62688', '97670', '52415', '13384', '28195', '82904', '77665', '25440', '14726', '98992', '33853', '53264', '47153', '45068', '93119', '91001', '66349', '77294', '16940', '28841', '68629', '32897', '95444', '62481', '68840', '32916', '18547', '82339', '53501', '31853', '70838', '16885', '34317', '36289', '44015', '10133', '29497', '14808', '33322', '80497', '95200', '62107', '33096', '81470', '54559', '14285', '42133', '38043', '76709', '47009', '97936', '98382', '71577', '34443', '22185', '77089', '79972', '30085', '97580', '54991', '58548', '51592', '56068', '88585', '97329', '14118', '63792', '82293', '61533', '78473', '92935', '86626', '18702', '27045', '78567', '80107', '93419', '97831', '83457', '63762', '50390', '30426', '66497', '95987', '21223', '25469', '67895', '69106', '64626', '39729', '93056', '59690', '23311', '33600', '58576', '69004', '28518', '77529', '25305', '60071', '66631', '70446', '26783', '23818', '36639', '34178', '66373', '83576', '40567', '32074', '39492', '71929', '35524', '66579', '24298', '46678', '52906', '20235', '90247', '89795', '89600', '27161', '65915', '63005', '16105', '22946', '75611', '40317', '53551', '97760', '81704', '57305', '87737', '50771', '60966', '98631', '16691', '11020', '32558', '43832', '86646', '12223', '49452', '30471', '20413', '74402', '19061', '92949', '14179', '73118', '26447', '85882', '98294', '98740', '30337', '48534', '80462', '93035', '38143', '13641', '92257', '70481', '27631', '40674', '78494', '87824', '70213', '90077', '77180', '99383', '32776', '70434', '84894', '74245', '16251', '80439', '89043', '25808', '80698', '52798', '21629', '59657', '83274', '42234', '36226', '49622', '54050', '34017', '83754', '85220', '48118', '84077', '48479', '40746', '33670', '77845', '59895', '95617', '93402', '97658', '48474', '11746', '30412', '94913', '88710', '90308', '35958', '40471', '51613', '11519', '28679', '41642', '52687', '78450', '49383', '12987', '13087', '55842', '67852', '71717', '15222', '86725', '26435', '10057', '11650', '30811', '62929', '20490', '23902', '53591', '51153', '53994', '16774', '63476', '87342', '24454', '25338', '80515', '10719', '54273', '25944', '64322', '15618', '83966', '47741', '66578', '35726', '91440', '81967', '39471', '50560', '44825', '50605', '81733', '32322', '46881', '16833', '35133', '25729', '76643', '85971', '29157', '20506', '49479', '80858', '32748', '43949', '50878', '17890', '67281', '98345', '85147', '52548', '16721', '29927', '92582', '94644', '57823', '29563', '23667', '31757', '75502', '22202', '96871', '30411', '79939', '33856', '48939', '96697', '35505', '99194', '85538', '15823', '31404', '65143', '59746', '18560', '24918', '16882', '73201', '88054', '90696', '64837', '30524', '77415', '77651', '54373', '62041', '15790', '48035', '92958', '43641', '49032', '55827', '39256', '57632', '79688', '40756', '21017', '62787', '39364', '16462', '10670', '97695', '16868', '51995', '70272', '33714', '29481', '39784', '68826', '22866', '47639', '88599', '29916', '78819', '24621', '45879', '81184', '15197', '98441', '17898', '21536', '66442', '78859', '75054', '57591', '11402', '84321', '85088', '76245', '79079', '97209', '28057', '59941', '46843', '16893', '72875', '35688', '23240', '23515', '53913', '58582', '22797', '66938', '49045', '11283', '67167', '93727', '93002', '76807', '18240', '42514', '59555', '39309', '37598', '56657', '83594', '18151', '69026', '50856', '21151', '67144', '59598', '55199', '57630', '21315', '72688', '50316', '45735', '82529', '71844', '99186', '41985', '43966', '96542', '94465', '81087', '72518', '57552', '93824', '97291', '29558', '76238', '34887', '25625', '89781', '43835', '88186', '37896', '26623', '80969', '22986', '29730', '59858', '76860', '22784', '17283', '30478', '54841', '90305', '64652', '87399', '97847', '31562', '18355', '52805', '40752', '30846', '82617', '90802', '34447', '63591', '51085', '44578', '76433', '19165', '34301', '79313', '81457', '46954', '34099', '56760', '61023', '40399', '58552', '76555', '85014', '99844', '66764', '26961', '63914', '61110', '64673', '40489', '41163', '47088', '26387', '82685', '21647', '25259', '63892', '95357', '25720', '90396', '24498', '26889', '68320', '42879', '87207', '40821', '26844', '59875', '27391', '51379', '41929', '39627', '68677', '27150', '41247', '58670', '49942', '96501', '83018', '90181', '62822', '43712', '55816', '20570', '74066', '55515', '35033', '38370', '39167', '18696', '65340', '71504', '21829', '28660', '60881', '56493', '11156', '14512', '88097', '42410', '21095', '40152', '74376', '98440', '73784', '31480', '43269', '98465', '74744', '16097', '93004', '44210', '66826', '69586', '37925', '85087', '65285', '52026', '19813', '62509', '62905', '32524', '41059', '55432', '13720', '34571', '46159', '79023', '22102', '47242', '15286', '31845', '80016', '43212', '69277', '49358', '31668', '35139', '57228', '12748', '36701', '97704', '34943', '33566', '93589', '83184', '45834', '75507', '77403', '40574', '24967', '26460', '81929', '31062', '69417', '95856', '51231', '48645', '80930', '27379', '36646', '79970', '59603', '48266', '17564', '25543', '30007', '26180', '13566', '18518', '24288', '71756', '31315', '79718', '80869', '47806', '97780', '45761', '24726', '68459', '19091', '11271', '18220', '38765', '43631', '17387', '47014', '33810', '32644', '91292', '48175', '58389', '95547', '55081', '82217', '63699', '59415', '56010', '22354', '76403', '18562', '72471', '80705', '60054', '95720', '27133', '93762', '26169', '19183', '72010', '61704', '60393', '88788', '46601', '94383', '88321', '28428', '59802', '69454', '21614', '55959', '97694', '76063', '40221', '81677', '29954', '51869', '50482', '62842', '83731', '91953', '88903', '10420', '62826', '14306', '70231', '37572', '57645', '70296', '21206', '52021', '73062', '39762', '43202', '44366', '39469', '94885', '99111', '90223', '76330', '58948', '16387', '78769', '60062', '53758', '90461', '82751', '33835', '73571', '24172', '46418', '86078', '35248', '56032', '17338', '25082', '80103', '98184', '12101', '21313', '74392', '33745', '38747', '62000', '56408', '48099', '70052', '91603', '49498', '84043', '16666', '78352', '47456', '92587', '16437', '41461', '66163', '48537', '79580', '65485', '71428', '28564', '39379', '16545', '62665', '91447', '90467', '59585', '76303', '78252', '81221', '41467', '19179', '79302', '47082', '30880', '45914', '20989', '49234', '97916', '81899', '75342', '83578', '33058', '69659', '90485', '38942', '12809', '33913', '20339', '61642', '43827', '47749', '69390', '57331', '92226', '66385', '32094', '66982', '60981', '39663', '78479', '86690', '58201', '12178', '96924', '49763', '72490', '58557', '60523', '55248', '42611', '85858', '42343', '48516', '98016', '93549', '61006', '95013', '55967', '30245', '47251', '45045', '63416', '99821', '14758', '95958', '27795', '76381', '31114', '98659', '88271', '39225', '20691', '12344', '87289', '77285', '22303', '35216', '52502', '94623', '68223', '73814', '56521', '46819', '28166', '22373', '75157', '80651', '45258', '22243', '74396', '65862', '38709', '31180', '70708', '78864', '64659', '12333', '47176', '94161', '61549', '47988', '32028', '88285', '42115', '71094', '56571', '49262', '58896', '76519', '73769', '21610', '61831', '38393', '46844', '60283', '72139', '98230', '12762', '56734', '13105', '96559', '80913', '44691', '99942', '31745', '72077', '39320', '15835', '13425', '62376', '72880', '84562', '82745', '49940', '94815', '62557', '41061', '20976', '16736', '37213', '35995', '94396', '64203', '23916', '84055', '92712', '98054', '98324', '51015', '38450', '75964', '13049', '94898', '85051', '64664', '21487', '50275', '97189', '18445', '87655', '80694', '77585', '88741', '26490', '67824', '75575', '47621', '26424', '73021', '87479', '42541', '36244', '54209', '84029', '77406', '49221', '65680', '87033', '64273', '38360', '50999', '37859', '70290', '94120', '76440', '59070', '20983', '51499', '86981', '77139', '38954', '82930', '66781', '83788', '24698', '90338', '16051', '62650', '63219', '69534', '38374', '12391', '91941', '18787', '85190', '32729', '37020', '51181', '23809', '92371', '45657', '38952', '92887', '70000', '61334', '68552', '25291', '55579', '95400', '71845', '46933', '53137', '54970', '67009', '83491', '89952', '65778', '38416', '50661', '26243', '66090', '67020', '24129', '29022', '95412', '54722', '43312', '79531', '59639', '15135', '37263', '33057', '50260', '98793', '62585', '84737', '27617', '91845', '96986', '15354', '90169', '42783', '74212', '17625', '59001', '16067', '22137', '78581', '97413', '56277', '43150', '55715', '95923', '11854', '49813', '18029', '35983', '52625', '89474', '48143', '21106', '43620', '93767', '74924', '90166', '37149', '15892', '69022', '76038', '27350', '22492', '79242', '78539', '85257', '76044', '69551', '42118', '16142', '20222', '86758', '99668', '92129', '10008', '10009', '65110', '60091', '59773', '12084', '89906', '25970', '22260', '90679', '30804', '31141', '17191', '90804', '64448', '72660', '36194', '45136', '98434', '78577', '79226', '17914', '72645', '13047', '67980', '66355', '78174', '86688', '58559', '22062', '99831', '17082', '71869', '46858', '37068', '36865', '98521', '45171', '26199', '67985', '25990', '52835', '97581', '43241', '91741', '82598', '18456', '81765', '45932', '29999', '40904', '73623', '18821', '29354', '21790', '45346', '65741', '47667', '99400', '92670', '94910', '11188', '41487', '83083', '21728', '87400', '76034', '53108', '10619', '94663', '99832', '34942', '80716', '56813', '17319', '77827', '57616', '31616', '99796', '15936', '79130', '49442', '34660', '32275', '41131', '18065', '13761', '64242', '92402', '77408', '16742', '21148', '58011', '84202', '20554', '11007', '34459', '41226', '35677', '37225', '23214', '68801', '29701', '22371', '26572', '21791', '75988', '58203', '50543', '37565', '61734', '15636', '70917', '41379', '38062', '21970', '10744', '46591', '38296', '29351', '66127', '52720', '48988', '70368', '30844', '75880', '19306', '32451', '80776', '29297', '63287', '22174', '80374', '74400', '59326', '93071', '59625', '16743', '32810', '25519', '10825', '35109', '95224', '60096', '65856', '28831', '94688', '84493', '14486', '61756', '19847', '20646', '32534', '39124', '36049', '85278', '51609', '61599', '40202', '56391', '12754', '33110', '15548', '30740', '51603', '41737', '30093', '61846', '90130', '43289', '68660', '17119', '36059', '33091', '48299', '22907', '65112', '54898', '41778', '81307', '94152', '32894', '22144', '72124', '27502', '79577', '53784', '85092', '42523', '88941', '24863', '23664', '64220', '31282', '16191', '13751', '56036', '38294', '77559', '89350', '53855', '27456', '35183', '41918', '82917', '22712', '61578', '89078', '30838', '13075', '27819', '53575', '36137', '63168', '61737', '29648', '46168', '57440', '82398', '57426', '92379', '13064', '54030', '15251', '23366', '55276', '24340', '93258', '82331', '32786', '62075', '46699', '54897', '38886', '17578', '23084', '60451', '42212', '71100', '81253', '93754', '81657', '21267', '85133', '15876', '93133', '21767', '95937', '49345', '95645', '32303', '53781', '54858', '19659', '81801', '23241', '19145', '89868', '55116', '96547', '81738', '70014', '69656', '45574', '86718', '32143', '56208', '77672', '32097', '34708', '42064', '92835', '93937', '40951', '82256', '12795', '62079', '84693', '45622', '78670', '98344', '53564', '10734', '75100', '88928', '33684', '98709', '34349', '49059', '63195', '66981', '93456', '84884', '15375', '65536', '18183', '90341', '19819', '79379', '95600', '19520', '24910', '15435', '25835', '42031', '73300', '25214', '29523', '31311', '54789', '26246', '51203', '49728', '93870', '33919', '70376', '55902', '37239', '43876', '82983', '20608', '13664', '14929', '93446', '74011', '66125', '24170', '58037', '35566', '15402', '96437', '38005', '89246', '75614', '61195', '87488', '55287', '98127', '60753', '53105', '48712', '20801', '31639', '70158', '98967', '21552', '34523', '61717', '27816', '58168', '77748', '87713', '36520', '41672', '97923', '53357', '33845', '57872', '12336', '15119', '68181', '26284', '69815', '45781', '11396', '42155', '86223', '18206', '70317', '20038', '86351', '95149', '41545', '52581', '65703', '64122', '55128', '83330', '81309', '88261', '51341', '91451', '36039', '65385', '31527', '98605', '88613', '46310', '85997', '52858', '44884', '20840', '61602', '31500', '84630', '83758', '45637', '67248', '58734', '33592', '59318', '57325', '81718', '69725', '26636', '66023', '44307', '71662', '29166', '49299', '83601', '32309', '42205', '29176', '62623', '46526', '40438', '30390', '24259', '46508', '34334', '96009', '45025', '43414', '51029', '56873', '84543', '84835', '79923', '50330', '98131', '77364', '13624', '44056', '86231', '81124', '73241', '99495', '32011', '17258', '24755', '65473', '41240', '70260', '49945', '79727', '59684', '98013', '41092', '29727', '29839', '35003', '77583', '69455', '94310', '41844', '73548', '98165', '65781', '11027', '57174', '44394', '26787', '87021', '30284', '46697', '88326', '17756', '44067', '81517', '83941', '19648', '43615', '90040', '86359', '42454', '73770', '52818', '74577', '64868', '65816', '60058', '40415', '35025', '33761', '27420', '81048', '99573', '59651', '33627', '47565', '28371', '28503', '14917', '64869', '44418', '78033', '39720', '34899', '72677', '52878', '56815', '84594', '20512', '77799', '36791', '65780', '66785', '97052', '67265', '65440', '21570', '41827', '38231', '65296', '12260', '70193', '19289', '39623', '61240', '50431', '31996', '85036', '47866', '59102', '78443', '79448', '21063', '77786', '18448', '83182', '71079', '84287', '50050', '37753', '32399', '21867', '60256', '74762', '13618', '84452', '13841', '52863', '31491', '72641', '48081', '29651', '57660', '71552', '77225', '94908', '90879', '11776', '75175', '40325', '18252', '36571', '89088', '81856', '88729', '78216', '38394', '61419', '97204', '27553', '47442', '92547', '22718', '33148', '14396', '20092', '71937', '12897', '52515', '32036', '12117', '52829', '84326', '82553', '94226', '39940', '51317', '14005', '25373', '51687', '14336', '65203', '26174', '28924', '47852', '44514', '51659', '51438', '52570', '51033', '86236', '63371', '28517', '11823', '22662', '52979', '40262', '68647', '87056', '86753', '81793', '76596', '45233', '71737', '17605', '64952', '58211', '84236', '49569', '73980', '58418', '89233', '63880', '81605', '85812', '76378', '42081', '41114', '75196', '18455', '25430', '55997', '59336', '52432', '64544', '37552', '57355', '68305', '87097', '54674', '87329', '68297', '20238', '73626', '27243', '81548', '49777', '51684', '98174', '75090', '39243', '17619', '33449', '63854', '32580', '57652', '28562', '65599', '94939', '13670', '81321', '44199', '65270', '23255', '93360', '41993', '63480', '14283', '23593', '12532', '37450', '27374', '41955', '74529', '25339', '28677', '20500', '26865', '94323', '40731', '91855', '17174', '51230', '51194', '65618', '10864', '99671', '25390', '73488', '91799', '29016', '46424', '18964', '81430', '58360', '71819', '64066', '45799', '70250', '87873', '79269', '14668', '10161', '14318', '23112', '29977', '49407', '70977', '79641', '42502', '66566', '17531', '82164', '49109', '25007', '88716', '91409', '91450', '74372', '47998', '28156', '49499', '18103', '87405', '40019', '12719', '16450', '12310', '24575', '71433', '99078', '43388', '42778', '11950', '84359', '27416', '35902', '74330', '40470', '12052', '41007', '67428', '25533', '74318', '35874', '12789', '19303', '60929', '29049', '80826', '93365', '13966', '75968', '63235', '90806', '99766', '15485', '51617', '40054', '49864', '32504', '64423', '58318', '99197', '42594', '62926', '35683', '42883', '43409', '47552', '10987', '30122', '68505', '85878', '13874', '66185', '91706', '83314', '58476', '57412', '63305', '74015', '51497', '89826', '98495', '19155', '59086', '53328', '34698', '79227', '27769', '72836', '76422', '40934', '58847', '73619', '17581', '63140', '12887', '49816', '89257', '53966', '88646', '13683', '26402', '40662', '64330', '12952', '73041', '60323', '36425', '47720', '76898', '80635', '99547', '70550', '41538', '67585', '48185', '24403', '35430', '42402', '75391', '51146', '41659', '74411', '80505', '20144', '16502', '91468', '59665', '32374', '81082', '76327', '21905', '22263', '81640', '75253', '33861', '78274', '90303', '44577', '85715', '27890', '78055', '30006', '43764', '86499', '41278', '90853', '82873', '69013', '23227', '41289', '21560', '49028', '16886', '91612', '71941', '90426', '98962', '19119', '99724', '10867', '17796', '51509', '70493', '99978', '81273', '26425', '14263', '41272', '28949', '15635', '12842', '41523', '96114', '45920', '19126', '94132', '82666', '77332', '48976', '25487', '15839', '14684', '62457', '52401', '81827', '97308', '86824', '30893', '55177', '72508', '25738', '56303', '92156', '66862', '50949', '19058', '82591', '61319', '13114', '65130', '52042', '20005', '79563', '74912', '82193', '14902', '15568', '34477', '43806', '77521', '31213', '73016', '19886', '49519', '91705', '92335', '61158', '96775', '81896', '78431', '56826', '64933', '71246', '94587', '60140', '46961', '23495', '64170', '90619', '73258', '80183', '81405', '69148', '44383', '82657', '31194', '15558', '61875', '60719', '99554', '31963', '92973', '18315', '27147', '17007', '67302', '81502', '78807', '47202', '88187', '70403', '29899', '39362', '83523', '31191', '17302', '79276', '72318', '61248', '80349', '53375', '33920', '50597', '18802', '39777', '18097', '28407', '47498', '83833', '10085', '79390', '77392', '29381', '51906', '59464', '91057', '54021', '44811', '26267', '31485', '63171', '36555', '45130', '91264', '91889', '78924', '15888', '74520', '36705', '53285', '45588', '90425', '74551', '94286', '55870', '76661', '49335', '17480', '20416', '75946', '54013', '83376', '28912', '63466', '98896', '26404', '81683', '42482', '23114', '39480', '79963', '40606', '30593', '55005', '44471', '97924', '66869', '86635', '19907', '36284', '62840', '19543', '21485', '92284', '39447', '65400', '27889', '54522', '65323', '58703', '56989', '20619', '97490', '43528', '76467', '66351', '33019', '46049', '48637', '80192', '91816', '39036', '61363', '52370', '23564', '64188', '64374', '89366', '46048', '17197', '70133', '99246', '16183', '27991', '20030', '69790', '25377', '71268', '18186', '80706', '65182', '47078', '58215', '82672', '23940', '47815', '85553', '82023', '43619', '58934', '32200', '97312', '74555', '11494', '69898', '24415', '31242', '97891', '24493', '99289', '62612', '35651', '62393', '40131', '24151', '70862', '92373', '84702', '19954', '81420', '99161', '61904', '23193', '82730', '10054', '46365', '40597', '34900', '61760', '51828', '55574', '15570', '74840', '36817', '86742', '21220', '36578', '16410', '11305', '67813', '88705', '45985', '61109', '32697', '12136', '61457', '21177', '68791', '91187', '83283', '75870', '24188', '24163', '16452', '78604', '20456', '78758', '46437', '72873', '72323', '37089', '80158', '53020', '51041', '78456', '44879', '10980', '91935', '78573', '55009', '99396', '45686', '61507', '24462', '61901', '41145', '59861', '96366', '67511', '12840', '13316', '66452', '82616', '74541', '17773', '53980', '19432', '37123', '24268', '41812', '70993', '73227', '85714', '53393', '22388', '77680', '74550', '13602', '78188', '84118', '12327', '79802', '44031', '94418', '26495', '54316', '86698', '17153', '39156', '47224', '86757', '99208', '15589', '24235', '20257', '97943', '42732', '11975', '12747', '72884', '42224', '36473', '85173', '76828', '76020', '48232', '11628', '83202', '77459', '88200', '73197', '28037', '75815', '84367', '50151', '19742', '36225', '37666', '16705', '28748', '85250', '84081', '37397', '86716', '66832', '61083', '63482', '87416', '95338', '82921']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "random.shuffle(data_set_off)\n",
        "random.shuffle(data_set_not)\n",
        "\n",
        "offensive_test = []\n",
        "not_test = []\n",
        "\n",
        "print(len(data_set_off))\n",
        "print(len(data_set_not))\n",
        "split_dataset = np.split(data_set_off, [int(len(data_set_off)*0.7)])\n",
        "split_dataset2 = np.split(data_set_not, [int(len(data_set_not)*0.7)])\n",
        "\n",
        "print(len(split_dataset[0]))\n",
        "print(len(split_dataset2[0]))\n",
        "print(\"test sizes:\")\n",
        "print(len(split_dataset[1]))\n",
        "print(len(split_dataset2[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEF8nKdp2VC_",
        "outputId": "4bff9e81-c2e5-4eb8-92dd-16badf718428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2209\n",
            "4411\n",
            "1546\n",
            "3087\n",
            "test sizes:\n",
            "663\n",
            "1324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import translators as ts\n",
        "\n",
        "index = 0\n",
        "for tweet in split_dataset[0]:\n",
        "  with open(\"twitterDataset/train/off/\"+str(index)+\".txt\",\"w\") as f:\n",
        "    f.write(tweet)\n",
        "  index+=1\n",
        "\n",
        "  translated = ts.google(tweet, to_language='fr')\n",
        "  augmented = ts.sogou(translated)\n",
        "  with open(\"twitterDataset/train/off/\"+str(index)+\".txt\",\"w\") as f:\n",
        "    f.write(augmented)\n",
        "  index+=1\n",
        "\n",
        "index = 0\n",
        "for tweet in split_dataset2[0]:\n",
        "  with open(\"twitterDataset/train/not/\"+str(index)+\".txt\",\"w\") as f:\n",
        "    f.write(tweet)\n",
        "  index+=1\n",
        "\n",
        "  translated = ts.google(tweet, to_language='fr')\n",
        "  augmented = ts.sogou(translated)\n",
        "  with open(\"twitterDataset/train/not/\"+str(index)+\".txt\",\"w\") as f:\n",
        "    f.write(augmented)\n",
        "  index+=1\n",
        "\n",
        "index = 0\n",
        "for tweet in split_dataset[1]:\n",
        "  with open(\"twitterDataset/test/off/\"+str(index)+\".txt\",\"w\") as f:\n",
        "    f.write(tweet)\n",
        "  index+=1\n",
        "\n",
        "  translated = ts.google(tweet, to_language='fr')\n",
        "  augmented = ts.sogou(translated)\n",
        "  with open(\"twitterDataset/test/off/\"+str(index)+\".txt\",\"w\") as f:\n",
        "    f.write(augmented)\n",
        "  index+=1\n",
        "\n",
        "index = 0\n",
        "for tweet in split_dataset2[1]:\n",
        "  with open(\"twitterDataset/test/not/\"+str(index)+\".txt\",\"w\") as f:\n",
        "    f.write(tweet)\n",
        "  index+=1\n",
        "\n",
        "  translated = ts.google(tweet, to_language='fr')\n",
        "  augmented = ts.sogou(translated)\n",
        "  with open(\"twitterDataset/test/not/\"+str(index)+\".txt\",\"w\") as f:\n",
        "    f.write(augmented)\n",
        "  index+=1"
      ],
      "metadata": {
        "id": "AlnKpYW2euBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jupyter notebook adds extra folder which makes another class which fucks binaryloss\n",
        "!rm -r twitterDataset/train/.ipynb_checkpoints\n",
        "!rm -r twitterDataset/test/.ipynb_checkpoints"
      ],
      "metadata": {
        "id": "cEZEKCBt_t8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'twitterDataset/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "class_names = raw_train_ds.class_names\n",
        "print(class_names)\n",
        "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'twitterDataset/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'twitterDataset/test',\n",
        "    batch_size=batch_size)\n",
        "\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "_-WyHK7YfPyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b433862d-6055-4ed7-8f28-3bd8d62407dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9266 files belonging to 2 classes.\n",
            "Using 7413 files for training.\n",
            "['not', 'off']\n",
            "Found 9266 files belonging to 2 classes.\n",
            "Using 1853 files for validation.\n",
            "Found 3974 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX8FtlpGJRE6"
      },
      "source": [
        "## Loading models from TensorFlow Hub\n",
        "\n",
        "Here you can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.\n",
        "\n",
        "  - [BERT-Base](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3), [Uncased](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) and [seven more models](https://tfhub.dev/google/collections/bert/1) with trained weights released by the original BERT authors.\n",
        "  - [Small BERTs](https://tfhub.dev/google/collections/bert/1) have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\n",
        "  - [ALBERT](https://tfhub.dev/google/collections/albert/1): four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by sharing parameters between layers.\n",
        "  - [BERT Experts](https://tfhub.dev/google/collections/experts/bert/1): eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.\n",
        "  - [Electra](https://tfhub.dev/google/collections/electra/1) has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).\n",
        "  - BERT with Talking-Heads Attention and Gated GELU [[base](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1), [large](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1)] has two improvements to the core of the Transformer architecture.\n",
        "\n",
        "The model documentation on TensorFlow Hub has more details and references to the\n",
        "research literature. Follow the links above, or click on the [`tfhub.dev`](http://tfhub.dev) URL\n",
        "printed after the next cell execution.\n",
        "\n",
        "The suggestion is to start with a Small BERT (with fewer parameters) since they are faster to fine-tune. If you like a small model but with higher accuracy, ALBERT might be your next option. If you want even better accuracy, choose\n",
        "one of the classic BERT sizes or their recent refinements like Electra, Talking Heads, or a BERT Expert.\n",
        "\n",
        "Aside from the models available below, there are [multiple versions](https://tfhub.dev/google/collections/transformer_encoders_text/1) of the models that are larger and can yield even better accuracy, but they are too big to be fine-tuned on a single GPU. You will be able to do that on the [Solve GLUE tasks using BERT on a TPU colab](https://www.tensorflow.org/text/tutorials/bert_glue).\n",
        "\n",
        "You'll see in the code below that switching the tfhub.dev URL is enough to try any of these models, because all the differences between them are encapsulated in the SavedModels from TF Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2022-03-29T12:30:15.782203Z",
          "iopub.status.busy": "2022-03-29T12:30:15.782025Z",
          "iopub.status.idle": "2022-03-29T12:30:15.792580Z",
          "shell.execute_reply": "2022-03-29T12:30:15.792025Z"
        },
        "id": "y8_ctG55-uTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0d2872-af7c-480e-a77e-19db4783cc3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/google/electra_small/2\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ],
      "source": [
        "#@title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'electra_small'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WrcxxTRDdHi"
      },
      "source": [
        "## The preprocessing model\n",
        "\n",
        "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models discussed above, which implements this transformation using TF ops from the TF.text library. It is not necessary to run pure Python code outside your TensorFlow model to preprocess text.\n",
        "\n",
        "The preprocessing model must be the one referenced by the documentation of the BERT model, which you can read at the URL printed above. For BERT models from the drop-down above, the preprocessing model is selected automatically.\n",
        "\n",
        "Note: You will load the preprocessing model into a [hub.KerasLayer](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) to compose your fine-tuned model. This is the preferred API to load a TF2-style SavedModel from TF Hub into a Keras model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:30:15.795438Z",
          "iopub.status.busy": "2022-03-29T12:30:15.795041Z",
          "iopub.status.idle": "2022-03-29T12:30:18.992854Z",
          "shell.execute_reply": "2022-03-29T12:30:18.992262Z"
        },
        "id": "0SQi-jWd_jzq"
      },
      "outputs": [],
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4naBiEE_cZX"
      },
      "source": [
        "Let's try the preprocessing model on some text and see the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:30:18.996679Z",
          "iopub.status.busy": "2022-03-29T12:30:18.996239Z",
          "iopub.status.idle": "2022-03-29T12:30:19.160173Z",
          "shell.execute_reply": "2022-03-29T12:30:19.159548Z"
        },
        "id": "r9-zCzJpnuwS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "8cb91e83-bbb7-4408-8d9e-95fbeb4d667b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-4ff72b1aaa34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# text_test = ['this is such an amazing movie!']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext_preprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_preprocess_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_set' is not defined"
          ]
        }
      ],
      "source": [
        "text_test = iter(data_set).next()\n",
        "print(text_test)\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqL7ihkN_862"
      },
      "source": [
        "As you can see, now you have the 3 outputs from the preprocessing that a BERT model would use (`input_words_id`, `input_mask` and `input_type_ids`).\n",
        "\n",
        "Some other important points:\n",
        "- The input is truncated to 128 tokens. The number of tokens can be customized, and you can see more details on the [Solve GLUE tasks using BERT on a TPU colab](https://www.tensorflow.org/text/tutorials/bert_glue).\n",
        "- The `input_type_ids` only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input.\n",
        "\n",
        "Since this text preprocessor is a TensorFlow model, It can be included in your model directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKnLPSEmtp9i"
      },
      "source": [
        "## Using the BERT model\n",
        "\n",
        "Before putting BERT into your own model, let's take a look at its outputs. You will load it from TF Hub and see the returned values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:30:19.163550Z",
          "iopub.status.busy": "2022-03-29T12:30:19.163058Z",
          "iopub.status.idle": "2022-03-29T12:30:26.095648Z",
          "shell.execute_reply": "2022-03-29T12:30:26.094996Z"
        },
        "id": "tXxYpK8ixL34"
      },
      "outputs": [],
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:30:26.099587Z",
          "iopub.status.busy": "2022-03-29T12:30:26.098983Z",
          "iopub.status.idle": "2022-03-29T12:30:26.708358Z",
          "shell.execute_reply": "2022-03-29T12:30:26.707624Z"
        },
        "id": "_OoF9mebuSZc"
      },
      "outputs": [],
      "source": [
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm61jDrezAll"
      },
      "source": [
        "The BERT models return a map with 3 important keys: `pooled_output`, `sequence_output`, `encoder_outputs`:\n",
        "\n",
        "- `pooled_output` represents each input sequence as a whole. The shape is `[batch_size, H]`. You can think of this as an embedding for the entire text.\n",
        "- `sequence_output` represents each input token in the context. The shape is `[batch_size, seq_length, H]`. You can think of this as a contextual embedding for every token in the text.\n",
        "- `encoder_outputs` are the intermediate activations of the `L` Transformer blocks. `outputs[\"encoder_outputs\"][i]` is a Tensor of shape `[batch_size, seq_length, 1024]` with the outputs of the i-th Transformer block, for `0 <= i < L`. The last value of the list is equal to `sequence_output`.\n",
        "\n",
        "For the fine-tuning you are going to use the `pooled_output` array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDNKfAXbDnJH"
      },
      "source": [
        "## Define your model\n",
        "\n",
        "You will create a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer.\n",
        "\n",
        "Note: for more information about the base model's input and output you can follow the model's URL for documentation. Here specifically, you don't need to worry about it because the preprocessing model will take care of that for you.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:30:26.711934Z",
          "iopub.status.busy": "2022-03-29T12:30:26.711712Z",
          "iopub.status.idle": "2022-03-29T12:30:26.716508Z",
          "shell.execute_reply": "2022-03-29T12:30:26.715939Z"
        },
        "id": "aksj743St9ga"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs4yhFraBuGQ"
      },
      "source": [
        "Let's check that the model runs with the output of the preprocessing model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:30:26.719652Z",
          "iopub.status.busy": "2022-03-29T12:30:26.719155Z",
          "iopub.status.idle": "2022-03-29T12:30:33.483247Z",
          "shell.execute_reply": "2022-03-29T12:30:33.482586Z"
        },
        "id": "mGMF8AZcB2Zy"
      },
      "outputs": [],
      "source": [
        "classifier_model = build_classifier_model()\n",
        "# bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "# print(tf.sigmoid(bert_raw_result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTUzNV2JE2G3"
      },
      "source": [
        "The output is meaningless, of course, because the model has not been trained yet.\n",
        "\n",
        "Let's take a look at the model's structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:30:33.486539Z",
          "iopub.status.busy": "2022-03-29T12:30:33.486005Z",
          "iopub.status.idle": "2022-03-29T12:30:33.607583Z",
          "shell.execute_reply": "2022-03-29T12:30:33.606965Z"
        },
        "id": "0EmzyHZXKIpm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "c613cf46-8f2f-4b75-ffe9-823a02bf0609"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "tf.keras.utils.plot_model(classifier_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbUWoZMwc302"
      },
      "source": [
        "## Model training\n",
        "\n",
        "You now have all the pieces to train a model, including the preprocessing module, BERT encoder, data, and classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpJ3xcwDT56v"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "Since this is a binary classification problem and the model outputs a probability (a single-unit layer), you'll use `losses.BinaryCrossentropy` loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "metadata": {
        "id": "sNv2j-5-xjSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:30:33.611866Z",
          "iopub.status.busy": "2022-03-29T12:30:33.611329Z",
          "iopub.status.idle": "2022-03-29T12:30:33.621515Z",
          "shell.execute_reply": "2022-03-29T12:30:33.620990Z"
        },
        "id": "OWPOZE-L3AgE"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = [tf.metrics.BinaryAccuracy(), get_f1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77psrpfzbxtp"
      },
      "source": [
        "### Optimizer\n",
        "\n",
        "For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as [AdamW](https://arxiv.org/abs/1711.05101).\n",
        "\n",
        "For the learning rate (`init_lr`), you will use the same schedule as BERT pre-training: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (`num_warmup_steps`). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:30:33.624696Z",
          "iopub.status.busy": "2022-03-29T12:30:33.624232Z",
          "iopub.status.idle": "2022-03-29T12:30:33.628769Z",
          "shell.execute_reply": "2022-03-29T12:30:33.628248Z"
        },
        "id": "P9eP2y9dbw32"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = 0.1*int(num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqlarlpC_v0g"
      },
      "source": [
        "### Loading the BERT model and training\n",
        "\n",
        "Using the `classifier_model` you created earlier, you can compile the model with the loss, metric and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:30:33.631424Z",
          "iopub.status.busy": "2022-03-29T12:30:33.631013Z",
          "iopub.status.idle": "2022-03-29T12:30:33.640619Z",
          "shell.execute_reply": "2022-03-29T12:30:33.640151Z"
        },
        "id": "-7GPDhR98jsD"
      },
      "outputs": [],
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpBuV5j2cS_b"
      },
      "source": [
        "Note: training time will vary depending on the complexity of the BERT model you have selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:30:33.643759Z",
          "iopub.status.busy": "2022-03-29T12:30:33.643317Z",
          "iopub.status.idle": "2022-03-29T12:37:37.231432Z",
          "shell.execute_reply": "2022-03-29T12:37:37.230870Z"
        },
        "id": "HtfDFAnN_Neu",
        "outputId": "a827981b-9644-449a-9f9f-3382603c8197",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with https://tfhub.dev/google/electra_small/2\n",
            "Epoch 1/5\n",
            "232/232 [==============================] - 2221s 10s/step - loss: 0.5906 - binary_accuracy: 0.6748 - get_f1: 0.2366 - val_loss: 0.4642 - val_binary_accuracy: 0.7733 - val_get_f1: 0.5352\n",
            "Epoch 2/5\n",
            "232/232 [==============================] - 2210s 10s/step - loss: 0.4571 - binary_accuracy: 0.7835 - get_f1: 0.5797 - val_loss: 0.4287 - val_binary_accuracy: 0.8095 - val_get_f1: 0.6501\n",
            "Epoch 3/5\n",
            "232/232 [==============================] - 2209s 10s/step - loss: 0.3860 - binary_accuracy: 0.8218 - get_f1: 0.6793 - val_loss: 0.4157 - val_binary_accuracy: 0.8241 - val_get_f1: 0.6836\n",
            "Epoch 4/5\n",
            "194/232 [========================>.....] - ETA: 5:39 - loss: 0.3327 - binary_accuracy: 0.8502 - get_f1: 0.7326"
          ]
        }
      ],
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=train_ds,# data_set, y=list_of_labels,\n",
        "                               validation_data=val_ds,\n",
        "                               epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best training acc & f1 / val acc & f1 / test acc & f1\n",
        "- small_bert/bert_en_uncased_L-4_H-512_A-8/1, lr= 3e-5: 0.7186 & 0.3602 / 0.7573 & 0.4957 / 0.7698\n",
        "- electra_small, lr= 3e-5: 0.6792 & 0.1151 / 0.6950 & 0.1390 / 0.6860 & 0.1346 \n",
        "- electra_small, lr= 3e-4: 0.8468 & 0.7301 / 0.7829 & 0.6366 / 0.7755 & 0.6627"
      ],
      "metadata": {
        "id": "QmNyLkI3scgu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBthMlTSV8kn"
      },
      "source": [
        "### Evaluate the model\n",
        "\n",
        "Let's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:37:37.235151Z",
          "iopub.status.busy": "2022-03-29T12:37:37.234581Z",
          "iopub.status.idle": "2022-03-29T12:38:36.128910Z",
          "shell.execute_reply": "2022-03-29T12:38:36.128342Z"
        },
        "id": "slqB-urBV9sP"
      },
      "outputs": [],
      "source": [
        "loss, accuracy, f1 = classifier_model.evaluate(test_ds)\n",
        "\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'F1 Score: {f1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uttWpgmSfzq9"
      },
      "source": [
        "### Plot the accuracy and loss over time\n",
        "\n",
        "Based on the `History` object returned by `model.fit()`. You can plot the training and validation loss for comparison, as well as the training and validation accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:38:36.132081Z",
          "iopub.status.busy": "2022-03-29T12:38:36.131630Z",
          "iopub.status.idle": "2022-03-29T12:38:36.424503Z",
          "shell.execute_reply": "2022-03-29T12:38:36.424018Z"
        },
        "id": "fiythcODf0xo"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "# r is for \"solid red line\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzJZCo-cf-Jf"
      },
      "source": [
        "In this plot, the red lines represent the training loss and accuracy, and the blue lines are the validation loss and accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtn7jewb6dg4"
      },
      "source": [
        "## Export for inference\n",
        "\n",
        "Now you just save your fine-tuned model for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:38:36.428146Z",
          "iopub.status.busy": "2022-03-29T12:38:36.427717Z",
          "iopub.status.idle": "2022-03-29T12:38:42.015407Z",
          "shell.execute_reply": "2022-03-29T12:38:42.014764Z"
        },
        "id": "ShcvqJAgVera",
        "outputId": "ab1a06f8-f62c-438a-e400-66a241f01956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 124). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "dataset_name = 'twitterHS'\n",
        "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
        "\n",
        "classifier_model.save(saved_model_path, include_optimizer=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r twitterModel.zip twitterHS_bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWSIDc7h98td",
        "outputId": "8c309abe-bf94-4b5f-cc4b-859571d42c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: twitterHS_bert/ (stored 0%)\n",
            "  adding: twitterHS_bert/variables/ (stored 0%)\n",
            "  adding: twitterHS_bert/variables/variables.index (deflated 75%)\n",
            "  adding: twitterHS_bert/variables/variables.data-00000-of-00001 (deflated 7%)\n",
            "  adding: twitterHS_bert/assets/ (stored 0%)\n",
            "  adding: twitterHS_bert/assets/vocab.txt (deflated 53%)\n",
            "  adding: twitterHS_bert/saved_model.pb (deflated 92%)\n",
            "  adding: twitterHS_bert/keras_metadata.pb (deflated 85%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbI25bS1vD7s"
      },
      "source": [
        "Let's reload the model, so you can try it side by side with the model that is still in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:38:42.019271Z",
          "iopub.status.busy": "2022-03-29T12:38:42.018826Z",
          "iopub.status.idle": "2022-03-29T12:38:48.305286Z",
          "shell.execute_reply": "2022-03-29T12:38:48.304688Z"
        },
        "id": "gUEWVskZjEF0"
      },
      "outputs": [],
      "source": [
        "reloaded_model = tf.saved_model.load(saved_model_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "classify_text_with_bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}